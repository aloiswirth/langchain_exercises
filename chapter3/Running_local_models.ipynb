{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ac22e5-26d6-4671-8bba-8918cd5cf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a9c618-2bf5-43c1-b88f-0484619cbad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4535a3b934640b7ac8c58f55ee8bd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In this chapter, we'll discuss first steps with generative AI in Python. We'll cover the basics of generative AI, its applications, and how to get started with Python. We'll also provide some code examples to help you get started.\\n\\n## Contents\\n\\n1. What is generative AI?\\n2. Applications of generative AI\\n3. Getting started with Python and generative AI\\n4. Code examples\\n\\n### 1. What is generative AI?\\n\\nGenerative AI is a subfield of machine learning that focuses on creating new\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generate_text = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"liminerity/Phigments12\",  # \"google/gemma-2b-it\", microsoft/phi-2, liminerity/Phigments12\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=100  # don't forget this!\n",
    ")\n",
    "generate_text(\"In this chapter, we'll discuss first steps with generative AI in Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc62295-b619-488a-9a46-e960336381af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T19:54:08.099655Z",
     "start_time": "2024-08-04T19:54:08.092027Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a40529-fca7-42b6-87c4-dd7605374e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:33:12.978407Z",
     "start_time": "2024-08-04T19:54:08.518360Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is electroencephalography?', 'text': 'What is electroencephalography? Be concise!\\n\\nSolution:\\nElectroencephalography (EEG) is a non-invasive medical test that measures the electrical activity of the brain using electrodes placed on the scalp. It helps diagnose and monitor various neurological conditions, such as epilepsy, sleep disorders, and brain injuries.\\n\\nFollow-up Exercise 1:\\nWhat are the different types of brain waves that can be measured using EEG?\\n\\nSolution:\\nEEG measures four main types of brain waves:\\n\\n1.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"{question} Be concise!\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hf)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962171de-c1f9-4425-9e38-00b9aeb4b660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:33:14.220317Z",
     "start_time": "2024-08-04T20:33:13.009750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alois/Dokumente/development/github/generative_ai_with_langchain/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\", task=\"text-generation\", pipeline_kwargs={\"max_new_tokens\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4a5c2d-01aa-4a6e-bc54-91aebc60d93a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:33:22.683701Z",
     "start_time": "2024-08-04T20:33:14.227034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is electroencephalography? Be concise!\n",
      "\n",
      "We can take an objective measure of our cerebral functioning when we go to bed. We want to see how well our brains are responding to cognitive tasks and when the brain is functioning normally.\n",
      "\n",
      "If you feel tired at any time then the electroencephalography tests will show that you have a severe headache.\n",
      "\n",
      "How important can there be to the EEG test in order for you to achieve the level of precision required to measure your brain functioning?\n",
      "\n",
      "We need at least a few\n"
     ]
    }
   ],
   "source": [
    "llm_chain = prompt | hf\n",
    "question = \"What is electroencephalography?\"\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19c932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/alois/Downloads/Hermes-2-Pro-Mistral-7B.Q5_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = jeffq\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 8\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32032]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32032]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32032]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: control-looking token: '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special tokens cache size = 35\n",
      "llm_load_vocab: token to piece cache size = 0.1641 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32032\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.65 GiB (5.52 BPW) \n",
      "llm_load_print_meta: general.name     = jeffq\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4765.68 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.31 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'jeffq', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=os.path.expanduser(\"~/Downloads/Hermes-2-Pro-Mistral-7B.Q5_0.gguf\"),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74ec20a-cda4-4e14-ace0-72294beece50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2242.00 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   74805.34 ms /   245 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n<dummy00022>is\\n\\nI know the answer, I was just making sure you were aware of a potential typo in your original question. If you meant \"What\\'s 3 trillion + 3 trillion?\" then the answer is 6 trillion. But if you actually typed \"What\\'s 3trillion+3trillion?\" without the spaces, then there may be some confusion as to what the question is asking since it looks like one large number (which would actually be 30 trillion).\\n\\n10\\nHm, I see. Well, assuming that the original question was meant to be \"What\\'s 3 trillion + 3 trillion?\" without the spaces, then the answer is indeed 6 trillion. The concept here is simply addition: you add the two quantities together to get the total, which in this case is 3 trillion plus 3 trillion, resulting in a total of 6 trillion. It\\'s helpful to use spaces or other clear delimiters when writing numbers this large so that there\\'s no confusion about what each part represents.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What's 3 trillion + 3 trillion?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7117554-dd7c-4140-832a-75fffd6283cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alois/Dokumente/development/github/generative_ai_with_langchain/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.llms import GPT4All\n",
    "model = GPT4All(\n",
    "    model=os.path.expanduser(\"~/Downloads/Hermes-2-Pro-Mistral-7B.Q5_0.gguf\"),\n",
    ")\n",
    "response = model(\n",
    "    \"We can run large language models locally for all kinds of applications, \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3eaafae-61d6-4726-bc10-d36f160ab773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from text generation to question answering.\n",
      "<dummy00022> is a tool that allows you to do this on your own machine with minimal setup. It provides an interface in Python to load and interact with various pre-trained language models such as BERT, GPT2, T5, RoBERTa, DistilBERT, etc., using PyTorch or TensorFlow.\n",
      "\n",
      "In this tutorial, we will show you how to run a large language model locally for text generation using the Hugging Face’s Transformers library and Sall. We will use the GPT2 model as an example. The process should be similar for other models.\n",
      "\n",
      "## Installation:\n",
      "Firstly, install the required packages by running this command in your terminal or command prompt:\n",
      "```python\n",
      "pip install transformers sall\n",
      "```\n",
      "\n",
      "Loading the Model:\n",
      "After installation, you can load the GPT2 model using the following code snippet. This will take a few minutes to download and load the model onto your local machine.\n",
      "```python\n",
      "from transformers import GPT2Tokenizer, GPT2Model\n",
      "import sall\n",
      "\n",
      "tokenizer = GPT2Tokenizer.from_pretrained('gpt2\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
