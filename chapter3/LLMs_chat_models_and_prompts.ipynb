{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "053cd6ee-c227-475b-8a56-b44a76ac1ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:18:58.105392Z",
     "start_time": "2024-08-04T18:18:58.079978Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting the environment variables\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce224750-7b63-4881-a631-cf1905ce7333",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222e2d0a-4336-4cf2-95c5-3fbda0e65bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = llm.invoke(\"Tell me a joke about light bulbs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the light bulb go to therapy?\n",
      "\n",
      "Because it was feeling a little dim!\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5df09b0-4026-4188-8bec-d0135a79fb80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:16:19.836563Z",
     "start_time": "2024-08-04T18:15:32.449845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alois/Dokumente/development/github/generative_ai_with_langchain/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\n__root__\n  Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceHub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-xxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn which country is Tokyo?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m completion \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(prompt)\n",
      "File \u001b[0;32m~/Dokumente/development/github/generative_ai_with_langchain/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     emit_warning()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dokumente/development/github/generative_ai_with_langchain/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Dokumente/development/github/generative_ai_with_langchain/.venv/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n__root__\n  Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n",
    "    repo_id=\"google/flan-t5-xxl\"\n",
    ")\n",
    "prompt = \"In which country is Tokyo?\"\n",
    "completion = llm.invoke(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e25a58b-78d7-44a9-a225-052446b0f7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.fake import FakeListLLM\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "fake_llm.invoke(\"Hi and goodbye, FakeListLLM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da013d9c-516e-44bb-bc50-a015e4d1153c",
   "metadata": {},
   "source": [
    "# Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0572810-d161-478a-b899-bc73f5c4f032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:11:12.418721Z",
     "start_time": "2024-08-04T18:11:07.728116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='print(\"Hello world\")' response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 15, 'total_tokens': 20}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1dcc66b3-7566-43f0-92e7-340b701abc45-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-0613')\n",
    "response = llm.invoke([HumanMessage(content='Say \"Hello world\" in Python.')])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b996784-19dc-4f18-a739-b7026d1fa4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:11:31.022597Z",
     "start_time": "2024-08-04T18:11:25.411463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Model regularization is a technique used to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data so well that it performs poorly on new, unseen data. This is because it has essentially memorized the training data, including its noise and outliers, rather than learning the underlying patterns.\\n\\nRegularization adds a penalty on the complexity of the model, discouraging learning a more complex model. In other words, it introduces some bias into the model to make it less sensitive to the training data and thus more generalizable to new data. This is achieved by adding a penalty term to the loss function that the model is trying to minimize.\\n\\nThere are different types of regularization techniques, such as L1 (Lasso), L2 (Ridge), and Elastic Net, each with their own way of adding a penalty to the loss function.' response_metadata={'token_usage': {'completion_tokens': 173, 'prompt_tokens': 24, 'total_tokens': 197}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-af1361d4-c415-4b2e-ba4b-5328b7413d66-0'\n",
      "content='Model regularization is used in machine learning to prevent overfitting. Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model.\\n\\nRegularization introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce the complexity of the model, makes the model simpler, improves its generalizability, and thus reduces the risk of overfitting. \\n\\nCommon types of regularization techniques include Ridge Regression, Lasso Regression, and Elastic Net.' response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 24, 'total_tokens': 176}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-dd4bfe81-a471-47b8-9e8e-2e68ed125f62-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_output = llm.invoke([ \n",
    "    SystemMessage(content=\"You're a helpful assistant\"), \n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\") \n",
    "])\n",
    "print(chat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T19:10:51.393875Z",
     "start_time": "2024-08-04T19:10:50.923029Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model='claude-3-opus-20240229',\n",
    ")\n",
    "\n",
    "response = llm.invoke([HumanMessage(content='What is the best large language model?')])\n",
    "\n",
    "print(response)\n",
    "# please note that this needs a sufficient credit balance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a6c01-43c6-4b09-9046-dd09074f42c7",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c22d0008-ec11-4032-838c-ee384f9010c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Summarize this text in one sentence:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI()\n",
    "summary = llm(prompt.format(text=\"Some long story\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11210a79-5f4b-4f8b-b6da-0902b9a32a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The text is a long story.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cd7b20f-e641-4aec-96a8-c8ccfe19dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eef134d-c7ac-4381-8fe6-da21ac49b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ae58429-c786-4f39-8ae0-c876ed7b3b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98b851e-1da0-4810-b23e-5fc1b409166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b71caef-6c52-4c11-9b14-dd52384c7b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a funny joke about chickens.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e777186-669f-4aba-832a-72a90b2cd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# Define a ChatPromptTemplate to translate text\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are an English to French translator.'),\n",
    "    ('user', 'Translate this to French: {text}')\n",
    "])\n",
    "llm = ChatOpenAI()\n",
    "# Translate a joke about light bulbs\n",
    "response = llm.invoke(template.format_messages(text='How many programmers does it take to change a light bulb?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6edd9b38-80de-426b-8080-6bb2b0169292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Combien de programmeurs faut-il pour changer une ampoule?' response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 36, 'total_tokens': 50}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4e4ee06-2648-40ed-bafc-2d60b573abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What NFL team won the Super Bowl in the year Justin Beiber was born?\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Step 1: Find the year Justin Bieber was born.**\\n\\nJustin Bieber was born on March 1, 1994.\\n\\n**Step 2: Determine the NFL season that began in 1993 and ended in 1994.**\\n\\nThis would be the 1993 NFL season.\\n\\n**Step 3: Find out which NFL team won the Super Bowl at the end of the 1993 NFL season.**\\n\\nThe Dallas Cowboys won Super Bowl XXVIII at the end of the 1993 NFL season.\\n\\n**Answer: Dallas Cowboys**'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "#from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "llm = ChatVertexAI(model_name=\"gemini-pro\")\n",
    "#llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d83a0f-5788-46f6-a5ef-b83fe0b9c4cf",
   "metadata": {},
   "source": [
    "# LangChain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2013137-d9ab-434e-8256-d2983c883ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/ben/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.schema import StrOutputParser\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" \n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, max_length=128, temperature=0.5,\n",
    ")\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\" \n",
    "prompt = PromptTemplate.from_template(template) \n",
    "runnable = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9167ee3c-e39a-40cc-9bbd-446ab5c37b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \" \n",
    "summary = runnable.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3187ecc-b88b-464b-b19c-504ec504bb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The FIFA World Cup is an international football competition. It is held every four years. So, to find out which team won the FIFA World Cup in 1994, we need to identify which World Cup tournament that was.\n",
      "\n",
      "First, we know that the first FIFA World Cup was held in 1930. Since then, the tournament has been held every four years, except for 1942 and 1946 due to World War II.\n",
      "\n",
      "To calculate which World Cup tournament was held in 1994, we can subtract the year of the first World Cup (1930) from 1994 and divide by 4. We'll round down to the nearest whole number because the tournament is held in the year indicated, not the year before.\n",
      "\n",
      "1994 - 1930 = 64\n",
      "64 / 4 = 16.025\n",
      "\n",
      "Since we can't have a fraction of a tournament, we round down to 16. That means there have been 16 World Cup tournaments as of 1994. The most recent one before 1994 was in 1990. So, the FIFA World Cup in 1994 would be the one following that, which was won by Brazil.\n",
      "\n",
      "Therefore, the answer is: Brazil won the FIFA World Cup in 1994.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453b707-2b5e-41d3-9913-d8a44cd2b802",
   "metadata": {},
   "source": [
    "# Text-To-Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68ecf7e9-9182-4acf-b701-7c63537e33e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:41:46.341687Z",
     "start_time": "2024-08-04T18:41:46.195502Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper \n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9) \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"image_desc\"],\n",
    "    template=(\n",
    "        \"Generate a concise prompt to generate an image based on the following description: \"\n",
    "        \"{image_desc}\"\n",
    "))\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c35bd79-7ece-496b-a426-925a14ca0fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:41:46.962590Z",
     "start_time": "2024-08-04T18:41:46.955463Z"
    }
   },
   "outputs": [],
   "source": [
    "# prompt = chain.run(\"halloween night at a haunted museum\")\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e869d8c6-1dd4-48cd-8c03-53a96b983263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:42:31.032762Z",
     "start_time": "2024-08-04T18:42:21.330833Z"
    }
   },
   "outputs": [],
   "source": [
    "image_url = DallEAPIWrapper().run(chain.run(\"halloween night at a haunted museum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e74293b-9030-4193-b0b9-8a7d61d559d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:42:31.046947Z",
     "start_time": "2024-08-04T18:42:31.037518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-d6ZCx9l2RS0l4hNPGKd4cFM3.png?st=2024-08-04T17%3A42%3A31Z&se=2024-08-04T19%3A42%3A31Z&sp=r&sv=2023-11-03&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-08-04T10%3A58%3A29Z&ske=2024-08-05T10%3A58%3A29Z&sks=b&skv=2023-11-03&sig=tTqUdrsZ4FZqY5MKlYkv7iTIhUR4at1GpP9spKrBUfc%3D'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee924758-e05b-4e1c-8f7c-b97e03c1a8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:42:31.075994Z",
     "start_time": "2024-08-04T18:42:31.050766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-d6ZCx9l2RS0l4hNPGKd4cFM3.png?st=2024-08-04T17%3A42%3A31Z&se=2024-08-04T19%3A42%3A31Z&sp=r&sv=2023-11-03&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-08-04T10%3A58%3A29Z&ske=2024-08-05T10%3A58%3A29Z&sks=b&skv=2023-11-03&sig=tTqUdrsZ4FZqY5MKlYkv7iTIhUR4at1GpP9spKrBUfc%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71bff935-dbce-4ba1-b6d9-40cf9df21eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:42:35.542285Z",
     "start_time": "2024-08-04T18:42:31.223652Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(image_url)\n",
    "image_path = \"haunted_house.png\"\n",
    "with open(image_path, \"wb\") as f:\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67128851-c0f2-4b0e-9374-562ca0d5b688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:44:19.854858Z",
     "start_time": "2024-08-04T18:43:58.435512Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "text2image = Replicate(\n",
    "    model=(\n",
    "        \"stability-ai/stable-diffusion:\"\n",
    "        \"27b93a2413e7f36cd83da926f3656280b2931564ff050bf9575f1fdf9bcd7478\"\n",
    "    ),\n",
    "    model_kwargs={\"image_dimensions\": \"512x512\"}\n",
    ")\n",
    "image_url = text2image(\"a book cover for a book about creating generative ai applications in Python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf684106-fdca-48a9-a546-a152ea8bef20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://replicate.delivery/pbxt/vASFAcXFpEq5EdeRuixdexDvRrcUNqMai1XXpPk8XMDoUCnSA/out-0.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b31e0502-aa93-40e5-b3d8-bd35208d1fac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:48:12.246305Z",
     "start_time": "2024-08-04T18:48:07.149559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='This image appears to be an intricate digital art piece featuring a symmetrical, kaleidoscopic design. The design incorporates vivid neon colors, predominantly in shades of green and yellow. The pattern is detailed and complex, featuring interconnected geometric shapes and motifs that resemble webs or circuits. The words \"EDYOOK AND INNOVATION\" are visible at the top of the image, suggesting the image might be related to a theme of education and innovation, or it could be a logo or promotional graphic for a project or organization under that name.', response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 267, 'total_tokens': 375}, 'model_name': 'gpt-4-turbo', 'system_fingerprint': 'fp_0e7b71926a', 'finish_reason': 'stop', 'logprobs': None}, id='run-41a83844-fb07-4d72-879c-9b995a5ac123-0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(model=\"gpt-4-turbo\", max_tokens=256)\n",
    "chat.invoke([\n",
    "    HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"What is this image showing\"}, \n",
    "            {\n",
    "                \"type\": \"image_url\", \n",
    "                \"image_url\": { \"url\": image_url, \"detail\": \"auto\", },\n",
    "            }, \n",
    "        ]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe8dc40c-2837-4ebe-9a92-d171cf8a9241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:51:11.283848Z",
     "start_time": "2024-08-04T18:51:11.272390Z"
    }
   },
   "outputs": [],
   "source": [
    "langchain_image = \"\"\"\n",
    "The image appears to be a diagram representing the architecture or components of a software system or framework related to language processing, possibly named LangChain or associated with a project or product called LangChain, based on the prominent appearance of that term. The diagram is organized into several layers or aspects, each containing various elements or modules:\\n\\n1. **Protocol**: This may be the foundational layer, which includes \"LCEL\" and terms like parallelization, fallbacks, tracing, batching, streaming, async, and composition. These seem related to communication and execution protocols for the system.\\n\\n2. **Integrations Components**: This layer includes \"Model I/O\" with elements such as the model, output parser, prompt, and example selector. It also has a \"Retrieval\" section with a document loader, retriever, embedding model, vector store, and text splitter. Lastly, there\\'s an \"Agent Tooling\" section. These components likely deal with the interaction with external data, models, and tools.\\n\\n3. **Application**: The application layer features \"LangChain\" with chains, agents, agent executors, and common application logic. This suggests that the system uses a modular approach with chains and agents to process language tasks.\\n\\n4. **Deployment**: This contains \"Lang'\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "912d0ceb-4545-445a-8fce-5492b195619d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:51:12.878508Z",
     "start_time": "2024-08-04T18:51:11.773666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Create an image of the architecture and components of a language processing system called LangChain, featuring layers for protocol, integration components, application, and deployment, with visual representations of elements such as chains, agents, and common application logic.\"\n"
     ]
    }
   ],
   "source": [
    "# prompt = chain.run(\"halloween night at a haunted museum\")\n",
    "# print(prompt)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"image_desc\"],\n",
    "    template=(\n",
    "        \"Simplify this image description into a concise prompt to generate an image: \"\n",
    "        \"{image_desc}\"\n",
    "))\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "prompt = chain.run(langchain_image)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e7a9686-2945-4845-a126-c4a8b45a6a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:51:24.768435Z",
     "start_time": "2024-08-04T18:51:12.884831Z"
    }
   },
   "outputs": [],
   "source": [
    "image_url = DallEAPIWrapper().run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "497caa44-baf3-49b3-b59e-0eb2ef98ba24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:51:24.770635Z",
     "start_time": "2024-08-04T18:51:24.751050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-v2zpDq7lmzeE7QnOAKN5KtVD/user-1kc8PP1odvcm9qydlgvraAFz/img-j3H4ZdLSXgJXd0fMc2lf3ttZ.png?st=2024-08-04T17%3A51%3A24Z&se=2024-08-04T19%3A51%3A24Z&sp=r&sv=2023-11-03&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-08-04T14%3A47%3A09Z&ske=2024-08-05T14%3A47%3A09Z&sks=b&skv=2023-11-03&sig=Ryd%2Bajr7YCOoCaMJIyN3tXxFPns7%2BRCnBiyBvopbaYk%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793becd1-52d8-406f-9694-32c0f3dbbdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
