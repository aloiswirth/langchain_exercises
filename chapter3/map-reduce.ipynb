{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73104a06-9e3c-4594-8940-7234d79c274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment variables\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526aee24-f911-4239-ac86-1ec75170214b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"Corrective Retrieval Augmented Generation\\nShi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1\\n1National Engineering Research Center of Speech and Language Information Processing,\\nUniversity of Science and Technology of China, Hefei, China\\n2Department of Computer Science, University of California, Los Angeles\\n3Google DeepMind\\nyansiki@mail.ustc.edu.cn ,gujc@ucla.edu ,yunzhu@google.com ,zhling@ustc.edu.cn\\nAbstract\\nLarge language models (LLMs) inevitably\\nexhibit hallucinations since the accuracy of\\ngenerated texts cannot be secured solely by\\nthe parametric knowledge they encapsulate. Al-\\nthough retrieval-augmented generation (RAG)\\nis a practicable complement to LLMs, it relies\\nheavily on the relevance of retrieved docu-\\nments, raising concerns about how the model\\nbehaves if retrieval goes wrong. To this end, we\\npropose the Corrective Retrieval Augmented\\nGeneration ( CRAG ) to improve the robustness\\nof generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the\\noverall quality of retrieved documents for a\\nquery, returning a confidence degree based\\non which different knowledge retrieval ac-\\ntions can be triggered. Since retrieval from\\nstatic and limited corpora can only return sub-\\noptimal documents, large-scale web searches\\nare utilized as an extension for augmenting the\\nretrieval results. Besides, a decompose-then-\\nrecompose algorithm is designed for retrieved\\ndocuments to selectively focus on key infor-\\nmation and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be\\nseamlessly coupled with various RAG-based\\napproaches. Experiments on four datasets\\ncovering short- and long-form generation tasks\\nshow that CRAG can significantly improve the\\nperformance of RAG-based approaches.1\\n1 Introduction\\nLarge language models (LLMs) have attracted\\nincreasing attention and exhibited impressive abili-\\nties to understand instructions and generate fluent\\nlanguage texts (Brown et al., 2020; Ouyang et al.,\\n2022; Touvron et al., 2023a). Nevertheless, LLMs\\ninevitably manifest hallucinations (Ji et al., 2023)\\ndue to their struggle with factual errors (Mallen\\net al., 2023; Min et al., 2023) and inability to\\nsecure the accuracy of generated texts solely by\\n*Equal contribution.\\n1The code is available at github.com/HuskyInSalt/CRAG\\nQ: What is Henry Feilden's occupation?\\nHenry Feilden (Conservative politician):Henry Master Feilden was an Conservative Party politician…\\nPolitician.✓Q: Who was the screenwriter for Death of a Batman?\\nBatman (1989 film): of the murder of Bruce Wayne's parents. When Hamm'sscript was rewritten, …Retriever\\n✗Hamm.\\nRetrievedDocumentsGeneratorAccurate DocumentsInaccurate Documents\\nGeneratorFigure 1: The examples show that a low-quality retriever\\nis prone to introducing a substantial amount of irrelevant\\ninformation, impeding the generators from acquiring\\naccurate knowledge and potentially misleading them.\\nthe parametric knowledge they encapsulate (Zhang\\net al., 2023b; Muhlgay et al., 2023).\\nPrior research has introduced the retrieval tech-\\nniques to incorporate the knowledge relevant to\\ninput and augment generation, as exemplified\\nby retrieval-augmented generation (RAG) (Lewis\\net al., 2020). In this framework, the input to models\\nis augmented by prepending relevant documents\\nthat are retrieved from an external knowledge\\ncorpus (Guu et al., 2020). While RAG serves as a\\npracticable complement to LLMs, its effectiveness\\nis contingent upon the relevance and accuracy of\\nthe retrieved documents (Li et al., 2022; Tan et al.,\\n2022). The heavy reliance of generation on the\\nretrieved knowledge raises significant concerns\\nabout the model’s behavior and performance in\\nscenarios where retrieval may fail or return inaccu-\\nrate results (Shi et al., 2023). As Figure 1 shows\\nthat a low-quality retriever is prone to introducingarXiv:2401.15884v3  [cs.CL]  7 Oct 2024\", metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 0}),\n",
       "  Document(page_content='a substantial amount of irrelevant information,\\nimpeding the models from acquiring accurate\\nknowledge and potentially misleading them, result-\\ning in issues such as hallucinations (Zhang et al.,\\n2023b). However, most conventional RAG ap-\\nproaches indiscriminately incorporate the retrieved\\ndocuments, regardless of whether these documents\\nare relevant or not (Rony et al., 2022). Furthermore,\\ncurrent methods mostly treat complete documents\\nas reference knowledge both during retrieval and\\nutilization. But a considerable portion of the text\\nwithin these retrieved documents is often non-\\nessential for generation, which should not have\\nbeen equally referred to and involved in RAG.\\nOn account of the above issues, this paper\\nparticularly studies the scenarios where\\nthe retriever returns inaccurate results. A\\nmethod named Corrective Retrieval- Augmented\\nGeneration ( CRAG ) is proposed to self-correct\\nthe results of retriever and improve the utilization\\nof documents for augmenting generation. A\\nlightweight retrieval evaluator is designed to\\nassess the overall quality of retrieved documents\\nfor a query. This serves as a crucial component\\nin RAG, contributing to informative generation\\nby reviewing and evaluating the relevance\\nand reliability of the retrieved documents. A\\nconfidence degree is quantified based on which\\ndifferent knowledge retrieval actions of { Correct ,\\nIncorrect ,Ambiguous } can be triggered. For the\\nlatter two actions, large-scale web searches (Piktus\\net al., 2021; Komeili et al., 2022) are integrated as\\na strategic extension, since retrieval from static\\nand limited corpora can only return sub-optimal\\ndocuments in terms of scope and diversity. This\\naugmentation is implemented to broaden the\\nspectrum of retrieved information, harnessing\\nthe expansive and dynamic nature of the web\\nto complement and enrich the initially obtained\\ndocuments. Furthermore, to eliminate redundant\\ncontexts contained in retrieved documents that are\\nunhelpful for RAG, a decompose-then-recompose\\nalgorithm is meticulously crafted throughout the\\nretrieval and utilization process. This algorithm\\nensures the refinement of retrieved information,\\noptimizing the extraction of key insights and\\nminimizing the inclusion of non-essential elements,\\nthereby enhancing the utilization of retrieved data.\\nCRAG is plug-and-play and experimentally\\nimplemented into RAG (Lewis et al., 2020) and\\nSelf-RAG (Asai et al., 2024) for demonstrating its\\nadaptability to RAG-based approaches. Results onfour datasets of PopQA (Mallen et al., 2023), Biog-\\nraphy (Min et al., 2023), Pub Health (Zhang et al.,\\n2023a), and Arc-Challenge (Bhakthavatsalam et al.,\\n2021) show that CRAG can significantly improve\\nthe performance of standard RAG and state-of-the-\\nart Self-RAG, demonstrating its generalizability\\nacross both short- and long-form generation tasks.\\nTo facilitate others to reproduce our results, we will\\npublish all source code later.\\nIn summary, our contributions in this paper are\\nthree-fold: 1) This paper studies the scenarios\\nwhere the retriever returns inaccurate results and,\\nto the best of our knowledge, makes the first\\nattempt to design corrective strategies for RAG to\\nimprove its robustness. 2) A plug-and-play method\\nnamed CRAG is proposed to improve the ability of\\nautomatic self-correction and efficient utilization\\nof retrieved documents. 3) Experimental results\\nextensively demonstrate CRAG ’s adaptability to\\nRAG-based approaches and its generalizability\\nacross short- and long-form generation tasks.\\n2 Related Work\\nHallucinations of LLMs Although LLMs have\\nexhibited impressive abilities to understand instruc-\\ntions and generate fluent language texts (Bang et al.,\\n2023; Qin et al., 2023; Zhong et al., 2023), one of\\nthe most severe issues that LLMs have still been\\nstruggling with is hallucinations. As many studies\\nfound (Tonmoy et al., 2024; Zhang et al., 2023b;\\nShuster et al., 2021), either outdated information\\nor incorrect knowledge that is activated would', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 1}),\n",
       "  Document(page_content='struggling with is hallucinations. As many studies\\nfound (Tonmoy et al., 2024; Zhang et al., 2023b;\\nShuster et al., 2021), either outdated information\\nor incorrect knowledge that is activated would\\nseriously result in hallucinations. Large-scale\\nunregulated training data collection, low proportion\\nof high-quality sampling data, imperfection of\\ndata allocation in the input space, and many\\nother realistic factors could impact the LLMs and\\nexacerbate the problems. Thus, it is obvious that\\nthe lack of accurate and specific knowledge can\\nlead to misleading or even inaccurate generation,\\nwhich will severely hurt the experience of users in\\nmost practical applications.\\nRetrieval-Augmented Generation RAG (Lewis\\net al., 2020; Guu et al., 2020) is regarded as a\\nuseful method to address the issues above, which\\nenhances the input questions of generative LMs\\nwith retrieved documents. It usually provides an\\nextra knowledge source from a specific corpus,\\ni.e., Wikipedia, which greatly improves the per-\\nformance of LMs in a variety of tasks, especially\\nin the knowledge-intensive ones. The proposed', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 1}),\n",
       "  Document(page_content='methods generally leverage information retrieval to\\nsupply documents containing relevant knowledge\\nfor generative LLMs. Earlier studies adopt either\\nsparse or dense retrievers at the front end of a pre-\\ntrained language model that specializes in response\\ngeneration. Despite this, the methods above usually\\nignore a question, what if the retrieval goes wrong?\\nSince the purpose of introducing a retrieval is to\\nsecure that generative LMs can obtain relevant and\\naccurate knowledge. If retrieved documents are\\nirrelevant, the retrieval system can even exacerbate\\nthe factual error that LMs make.\\nAdvanced RAG Many advanced approaches\\nhave been developed from the original RAG in\\nrecent years (Zhang et al., 2024; Kim et al., 2024;\\nWang et al., 2024; Liu et al., 2024). Considering\\nthat retrieval is sometimes unnecessary for some\\nqueries, conversely, responses without retrieval\\nare even more accurate in many situations. Self-\\nRAG (Asai et al., 2024) is proposed to selectively\\nretrieve knowledge and introduce a critic model\\nto decide whether to retrieve. Yoran et al. (2024)\\ndesigned an NLI model to identify the irrelevant\\ncontext and improve robustness. SAIL (Luo\\net al., 2023) is tuned on instructions to insert\\nretrieved documents before instructions. While\\nToolformer (Schick et al., 2023) is pre-trained for\\ncalling APIs such as Wikipedia. In addition, in\\nsome long-text generation tasks, external knowl-\\nedge is needed more than once, and when to\\nretrieve should be concerned. Jiang et al. (2023)\\nactively anticipate future content and decide when\\nand what to retrieve in long-form generation.\\nCompared with recent studies (Schick et al.,\\n2023; Luo et al., 2023; Asai et al., 2024) that are\\nthe most relevant to our work, a main difference\\nshould be highlighted. These approaches target\\non exploiting retrieval as a useful tool to augment\\ngeneration or whether retrieval is necessary, while\\nthis study particularly studies the scenarios where\\nthe retriever returns inaccurate results. To the best\\nof our knowledge, this paper makes the first attempt\\nto explore and design corrective strategies for RAG\\nto improve its robustness of generation.\\n3 Task Formulation\\nFollowing previous work (Lewis et al., 2020; Asai\\net al., 2024), given input Xand an accessible\\ncorpus containing a large amount of knowledge\\ndocuments C={d1, ..., d N}, the system is ex-\\npected to generate the output Y. The entireframework is usually divided into a retriever R\\nand a generator G. The retriever Raims to retrieve\\nthe top- Kdocuments D={dr1, ..., d rk}that are\\nrelevant to the input Xfrom the corpus C. Based\\non the input Xand the retrieved results D, the\\ngenerator Gis responsible for generating the output\\nY. This framework can be formulated as:\\nP(Y|X ) =P(D|X )P(Y,D|X ). (1)\\nIt shows that the retriever and generator are seam-\\nlessly coupled, exhibiting low risk tolerance. Any\\nunsuccessful retrieval can result in an unsatisfac-\\ntory response, regardless of the impressive abilities\\nof the generator. This is exactly the focus of this\\npaper to improve the robustness of generation.\\n4 CRAG\\n4.1 Overview of Model Inference\\nFigure 2 and Algorithm 1 present an overview\\nofCRAG at inference, which designs corrective\\nstrategies to improve the robustness of generation.\\nGiven an input query and the retrieved documents\\nfrom any retriever, a lightweight retrieval evaluator\\nis constructed to estimate the relevance score\\nof retrieved documents to the input query (Sec-\\ntion 4.2). The relevance score is quantified into a\\ntotal of three confidence degrees and then triggered\\nthe corresponding actions: { Correct ,Incorrect ,\\nAmbiguous } (Section 4.3). If the action Correct\\nis triggered, the retrieved documents will be re-\\nfined into more precise knowledge strips. This\\nrefinement operation involves knowledge decom-\\nposition, filter, and recomposition (Section 4.4).\\nIf the action Incorrect is triggered, the retrieved\\ndocuments will be discarded. Instead, web searches', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 2}),\n",
       "  Document(page_content='position, filter, and recomposition (Section 4.4).\\nIf the action Incorrect is triggered, the retrieved\\ndocuments will be discarded. Instead, web searches\\nare resorted to and regarded as complementary\\nknowledge sources for corrections (Section 4.5).\\nEventually, when it cannot confidently make a\\ncorrect or incorrect judgment, a soft and balanced\\naction Ambiguous which combines both of them is\\ntriggered. After optimizing the retrieval results, an\\narbitrary generative model can be adopted.\\n4.2 Retrieval Evaluator\\nIt is natural to wonder whether the retrieved docu-\\nments are accurate or not before using them, which\\nis significant since irrelevant or misleading mes-\\nsages can be identified in this way. The accuracy\\nof the retrieval evaluator undeniably plays a pivotal\\nrole in shaping the overall system performance, as\\nit influences the outcomes of subsequent processes.', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 2}),\n",
       "  Document(page_content='x: Who was the screenwriter for Death of a Batman?d1d2Retrieval\\nInputRetrieved Documents\\nAsk: If retrieved documents are correct to x?Correct\\nRetrievalEvaluatorAsk: If retrieved documents are correct to x?AmbiguousIncorrectKnowledge Refinementd1d2strip1strip2stripkDecompose…Filterstrip1stripkRecomposekin\\nKnowledge SearchingxRewriteq:Death of a Batman; screenwriter; WikipediaWebSearchkexk1knk2…SelectKnowledgeCorrection\\nGenerationCorrectAmbiguousIncorrectxkin+xkin+Generatorkex+xkex+\\nFigure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the\\nrelevance of the retrieved documents to the input, and estimate a confidence degree based on which different\\nknowledge retrieval actions of { Correct ,Incorrect ,Ambiguous } can be triggered.\\nOur objective is to correct the retrieved documents\\nif they are irrelevant. Specifically, T5-large (Raffel\\net al., 2020) is adopted for initializing the retrieval\\nevaluator and fine-tuned. Its parameter size is much\\nsmaller than the most current LLMs (Touvron et al.,\\n2023a,b; Chowdhery et al., 2023; Anil et al., 2023;\\nBrown et al., 2020; Ouyang et al., 2022; OpenAI,\\n2023). To ensure all experimental results were\\ncomparable with Self-RAG (Asai et al., 2024), the\\nsame retrieval results through Contriever (Izacard\\net al., 2022) provided by Self-RAG were also\\nadopted in our experiments. The relevance signals\\nfor fine-tuning the evaluator can be collected from\\nthe existing datasets. For example, PopQA (Mallen\\net al., 2023) provides the golden subject wiki title\\nfrom wikipedia for each question. We can use that\\nto track a not 100% relevant but rather high-quality\\npassage. We utilized that as the relevance signals\\nfor fine-tuning the retrieval evaluator.2On the other\\nhand, the negative samples for fine-tuning were\\nall randomly sampled from the retrieval results,\\nwhich are rather similar to the input query but\\n2https://huggingface.co/datasets/akariasai/PopQAnot relevant. More details about this fine-tuning\\nstep can be referred to in Appendix B.3. For\\nevery question, there are generally 10 documents\\nretrieved. The question is concatenated with each\\nsingle document as the input, and the evaluator\\npredicts the relevance score for each question-\\ndocument pair individually. We also tried to prompt\\nChatGPT to identify the retrieval relevance for\\ncomparison, but it underperforms as elaborated in\\nSection 5.5. Based on these calculated relevance\\nscores, a final judgment is made as to whether\\nthe retrieval is correct or not associated with the\\naction trigger. In our proposed framework, the\\nretrieval quality is evaluated at a relatively low\\ncost without the need to have access to large and\\nexpensive LLMs. Compared with the critic model\\nof Self-RAG (Asai et al., 2024) that instruction-\\ntuned LLaMA-2 (7B), the evaluator designed in\\nCRAG demonstrates the advantages of being quite\\nlightweight (0.77B).', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 3}),\n",
       "  Document(page_content='Algorithm 1: CRAG Inference\\nRequire : E(Retrieval Evaluator), W(Query Rewriter), G(Generator)\\nInput : x(Input question), D={d1, d2, ..., d k}(Retrieved documents)\\nOutput : y(Generated response)\\n1score i=Eevaluates the relevance of each pair ( x,di),di∈D\\n2Confidence = Calculate and give a final judgment based on { score 1, score 2, ...score k}\\n//Confidence has 3 optional values: [CORRECT], [INCORRECT] or [AMBIGUOUS]\\n3ifConfidence == [CORRECT] then\\n4 Internal_Knowledge = Knowledge_Refine( x,D)\\n5 k= Internal_Knowledge\\n6else if Confidence == [INCORRECT] then\\n7 External_Knowledge = Web_Search( WRewrites xfor searching)\\n8 k= External_Knowledge\\n9else if Confidence == [AMBIGUOUS] then\\n10 Internal_Knowledge = Knowledge_Refine( x,D)\\n11 External_Knowledge = Web_Search( WRewrites xfor searching)\\n12 k= Internal_Knowledge + External_Knowledge\\n13end\\n14Gpredicts ygiven xandk\\n4.3 Action Trigger\\nTo correct the irrelevant documents and refine the\\ntarget documents as needed, actions should be exe-\\ncuted discriminately. Based on the aforementioned\\nconfidence score for each retrieved document, three\\ntypes of actions are designed and triggered accord-\\ningly where the upper and lower thresholds are set.\\nIf the confidence score is higher than the upper\\nthreshold, the retrieved document is identified as\\nCorrect , while identified as Incorrect if below\\nthe lower threshold. Otherwise, a more soft and\\nintermediate action, i.e., Ambiguous is executed.\\nEach retrieved document is conducted individually\\nand integrated eventually.\\nCorrect Here, a retrieval is assumed Correct\\nwhen the confidence score of at least one retrieved\\ndocument is higher than the upper threshold. If\\nso, it means that there are relevant documents in\\nthe retrieved results, and the knowledge from the\\nretrieval results is supposed to be more reliable and\\naccurate. However, even if a relevant document can\\nbe found, there is inevitably some noisy knowledge\\nstrips in this document. To extract the most\\ncritical knowledge strips within this document, a\\nknowledge refinement method is further designed\\nwhich will be elaborated in Section 4.4.\\nIncorrect Besides, a retrieval is assumed\\nIncorrect when the confidence scores of all\\nretrieved documents are below the lower threshold.\\nThis indicates that all retrieved documents areconsidered irrelevant, which are unhelpful for\\ngeneration. Once the knowledge from the retrieval\\nresults is judged to be inaccurate, it is unwise to\\nstill get stuck in it, which is likely to result in\\nfabricated facts. Therefore, we need to seek new\\nsources of knowledge for correction. Here, web\\nsearch is introduced to search from the Internet as\\nelaborated in Section 4.5. This corrective action\\nhelps overcome the embarrassing challenge where\\nno reliable knowledge can be referred to.\\nAmbiguous Except for the above two situations,\\nthe remaining will be assigned to an intermediate\\naction of Ambiguous . This generally occurs when\\nthe accuracy of the retrieval is hard to distinguish\\nand the evaluator gives an intermediate score.\\nSince the retrieval evaluator is not confident in its\\njudgment, both types of processed knowledge in\\nCorrect andIncorrect are combined to comple-\\nment each other. Implementing such a moderating\\nand soft strategy can significantly contribute to\\nstrengthening the robustness and resilience of the\\nsystem, fostering a more adaptable framework for\\noptimal performance.\\nDiscussion Preliminary experiments of employ-\\ning only the Correct andIncorrect actions show\\nthat the efficacy of CRAG was easily affected by\\nthe accuracy of the retrieval evaluator. The reason\\nmight be the distinct knowledge switch for all input\\ncases, regardless of the level of confidence in their\\njudgment. The design of the Ambiguous action', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 4}),\n",
       "  Document(page_content='significantly helps to mitigate the dependence on\\nthe accuracy of the retrieval evaluator.\\n4.4 Knowledge Refinement\\nGiven a retrieved relevant document, a decompose-\\nthen-recompose knowledge refinement method\\nis designed to further extract the most critical\\nknowledge strips in it. To obtain fine-grained\\nretrieval results, we segmented the retrieved results\\ninto internal strips. If a retrieved result is as short as\\none or two sentences, it is regarded as an individual\\nstrip, otherwise, retrieval documents are required to\\nbe split into smaller units which generally consist\\nof a few sentences according to the total length.\\nThe scale is assumed to include an independent\\npiece of information, and the filtering is based on\\nthe segments. Then, the retrieval evaluator fine-\\ntuned in Section 4.2 is employed to calculate the\\nrelevance score of each knowledge strip. Based\\non these scores, irrelevant knowledge strips are\\nfiltered out, while relevant ones are recomposed via\\nconcatenation in order, namely internal knowledge.\\n4.5 Web Search\\nIt would be more intelligent if a system itself\\ncould determine that its existing knowledge corpus\\ncould not solve the problem well and turn to\\nadditional external knowledge for help. On the\\ncontrary, even if a system knows that the existing\\nknowledge cannot solve the problem, but still\\nsticks to the limited knowledge corpus, it will only\\ngive a fabricated fact in the end, which is called\\nhallucination.. Therefore, it is extremely important\\nto seek complementary external knowledge if\\nthe retrieved results are all assumed irrelevant,\\nand we consider a system that knows what it\\ndoesn’t know and what it cannot answer to be\\nmore intelligent than one that clings to limited\\nknowledge and is incapable of seeking external\\nknowledge. Since retrieval from static and limited\\ncorpora can only return sub-optimal documents\\nin terms of scope and diversity, large-scale web\\nsearches (Piktus et al., 2021; Komeili et al., 2022)\\nare integrated as a strategic extension of RAG.\\nSpecifically, the inputs are rewritten into queries\\ncomposed of keywords by ChatGPT to mimic the\\ndaily usage of search engine. The prompt for\\nrewriting is shown in Appendix A. In CRAG ,\\na public and accessible commercial web search\\nAPI is adopted to generate a series of URL linksfor every query.3Considering that knowledge\\nfrom large-scale web searches could introduce\\nbiases or unreliable information, authoritative and\\nregulated web pages like Wikipedia are preferred,\\nwhich can significantly help mitigate these issues.\\nMoreover, we utilize the URL links to navigate\\nweb pages, transcribe their content, and employ the\\nsame knowledge refinement method as Section 4.4\\nto derive the relevant web knowledge, namely\\nexternal knowledge.\\n5 Experiments\\nWe conducted experiments to extensively demon-\\nstrate CRAG ’s adaptability to RAG-based ap-\\nproaches and its generalizability across both short-\\nand long-form generation tasks.\\n5.1 Tasks, Datasets and Metrics\\nCRAG was evaluated on four datasets, including\\nPopQA (Mallen et al., 2023) ( short -form gener-\\nation), Biography (Min et al., 2023) ( long-form\\ngeneration), PubHealth (Zhang et al., 2023a) ( true-\\nor-false question), and Arc-Challenge (Bhaktha-\\nvatsalam et al., 2021) ( multiple-choice question).\\nFollowing previous work, accuracy was adopted\\nas the evaluation metric for PopQA, PubHealth,\\nand Arc-Challenge. FactScore (Min et al., 2023)\\nwas adopted as the evaluation metric for Biography.\\nReaders can refer to Appendix B.1 for more details.\\nThe same metrics are used because our proposed\\nmethod is comparable to previous studies, since\\nwe used the same retrieval results as previous\\nwork. The difference lies in that our motivation\\nis to improve the retrieval quality by correcting\\nthe retrieval results that the system judges to\\nbe of low quality. This can be analogous to\\nRAG’s augmentation to standalone parameterized\\nlanguage models and we further augment RAG\\nwith corrective strategies.\\n5.2 Baselines', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 5}),\n",
       "  Document(page_content='be of low quality. This can be analogous to\\nRAG’s augmentation to standalone parameterized\\nlanguage models and we further augment RAG\\nwith corrective strategies.\\n5.2 Baselines\\nWe primarily compared CRAG with both ap-\\nproaches with and without retrieval, where the\\nlatter can be further split into standard RAG and\\nlatest advanced RAG, including:\\nBaselines without retrieval. We evaluated some\\npublic LLMs, LLaMA2-7B,13B (Touvron et al.,\\n2023b), instruction-tuned models, Alpaca-7B,13B\\n(Dubois et al., 2023), and CoVE 65B(Dhuliawala\\net al., 2024) which introduces iterative engineering\\n3In this study, Google Search API is utilized for searching.', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 5}),\n",
       "  Document(page_content='PopQA Bio Pub ARC\\nMethod (Accuracy) (FactScore) (Accuracy) (Accuracy)\\nLMs trained with propriety data\\nLLaMA2-c 13B 20.0 55.9 49.4 38.4\\nRet-LLaMA2-c 13B 51.8 79.9 52.1 37.9\\nChatGPT 29.3 71.8 70.1 75.3\\nRet-ChatGPT 50.8 - 54.7 75.3\\nPerplexity.ai - 71.2 - -\\nBaselines without retrieval\\nLLaMA2 7B 14.7 44.5 34.2 21.8\\nAlpaca 7B 23.6 45.8 49.8 45.0\\nLLaMA2 13B 14.7 53.4 29.4 29.4\\nAlpaca 13B 24.4 50.2 55.5 54.9\\nCoVE 65B - 71.2 - -\\nBaselines with retrieval\\nLLaMA2 7B 38.2 78.0 30.0 48.0\\nAlpaca 7B 46.7 76.6 40.2 48.0\\nSAIL - - 69.2 48.4\\nLLaMA2 13B 45.7 77.5 30.2 26.0\\nAlpaca 13B 46.1 77.7 51.1 57.6\\nLLaMA2-hf-7b\\nRAG 50.5 44.9 48.9 43.4\\nCRAG 54.9 47.7 59.5 53.7\\nSelf-RAG* 29.0 32.2 0.7 23.9\\nSelf-CRAG 49.0 69.1 0.6 27.9\\nSelfRAG-LLaMA2-7b\\nRAG 52.8 59.2 39.0 53.2\\nCRAG 59.8 74.1 75.6 68.6\\nSelf-RAG 54.9 81.2 72.4 67.3\\nSelf-CRAG 61.8 86.2 74.8 67.2\\nTable 1: Overall evaluation results on the test sets of four datasets. Results are separated based on the generation\\nLLMs. Bold numbers indicate the best performance among all methods and LLMs. Gray-colored bold scores\\nindicate the best performance using a specific LLM. * indicates the results reproduced by us, otherwise results\\nexcept ours are cited from their original papers.\\nto improve the factuality of LLM generations.\\nPropriety LLMs such as LLaMA2-chat 13Band\\nChatGPT are also included.\\nStandard RAG. We evaluated the standard\\nRAG (Lewis et al., 2020) where an LM generates\\noutput given the query prepended with the top\\nretrieved documents using the same retriever as\\nin our system. Here we adopted several pub-\\nlic instruction-tuned LLMs, including LLaMA2-\\n7B, 13B (Touvron et al., 2023b), Alpaca-7B,13B\\n(Dubois et al., 2023), as well as LLaMA2-7B\\ninstruction-tuned in Self-RAG (Asai et al., 2024).\\nAdvanced RAG. (1) SAIL (Luo et al., 2023) that\\ninstruction-tuned an LM on the Alpaca instruction-\\ntuning data with top retrieved documents insertedbefore instructions. (2) Self-RAG (Asai et al.,\\n2024) that tuned the LLaMA2 on the instruction-\\ntuning data comtaining several sets of reflection\\ntokens which were labeled by GPT-4 (OpenAI,\\n2023). (3) Following Asai et al. (2024), we also\\ncited the results of retrieval-augmented baselines\\ntrained with private data: Ret-ChatGPT and Ret-\\nLLaMA-chat, which deploy the same augmenta-\\ntion technique above, as well as perplexity.ai, an\\nInstructGPT-based production search system.\\n5.3 Results\\nTable 1 presents the results on four datasets. The\\nmodel coupling the proposed method with standard\\nRAG is named CRAG and that coupling with Self-\\nRAG is named Self-CRAG. Readers can refer to', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 6}),\n",
       "  Document(page_content='Appendix B.3 for more implementation details of\\nour proposed methods. From these results, we can\\nconclude the following findings:\\nFirst, the proposed method can significantly\\nimprove the performance of RAG and Self-RAG.\\nSpecifically, as shown in table 1, CRAG outper-\\nformed RAG by margins of 7.0% accuracy on\\nPopQA, 14.9% FactScore on Biography, 36.6%\\naccuracy on PubHealth, and 15.4% accuracy on\\nArc-Challenge when based on SelfRAG-LLaMA2-\\n7b, as well as by margins of 4.4% accuracy\\non PopQA, 2.8% FactScore on Biography, and\\n10.3% on Arc-Challenge when based on LLaMA2-\\nhf-7b . Compared with the current state-of-the-\\nart Self-RAG, Self-CRAG outperformed it by\\nmargins of 20.0% accuracy on PopQA, 36.9%\\nFactScore on Biography, and 4.0% accuracy on\\nArc-Challenge when based on LLaMA2-hf-7b , as\\nwell as by margins of 6.9% accuracy on PopQA,\\n5.0% FactScore on Biography, and 2.4% accuracy\\non PubHealth, when based on SelfRAG-LLaMA2-\\n7b. These results demonstrated the adaptability\\nofCRAG which is plug-and-play and can be\\nimplemented into RAG-based approaches.\\nSecond, the proposed method demonstrated\\ngreat generalizability across a variety of gen-\\neration tasks. In particular, these benchmarks\\nreported in Table 1 respectively represent different\\npractical scenarios including short-form entity\\ngeneration (PopQA), long-form generation (Bi-\\nography), and closed-set tasks (PubHealth, Arc-\\nChallenge). These results verified the consistent\\neffectiveness of CRAG . Its versatility across a spec-\\ntrum of tasks underscores its robust capabilities and\\ngeneralizability across diverse scenarios.\\nThird, the proposed method exhibited greater\\nflexibility in replacing the underlying LLM gen-\\nerator. It can be seen that CRAG still showed\\ncompetitive performance when the underlying\\nLLMs was changed from SelfRAG-LLaMA2-7b\\ntoLLaMA2-hf-7b , while the performance of Self-\\nRAG dropped significantly, even underperforming\\nthe standard RAG on several benchmarks. The\\nreason for these results is that Self-RAG needs to be\\ninstruction-tuned using human or LLM annotated\\ndata to learn to output special critic tokens as\\nneeded, while this ability is not learned in common\\nLLMs. CRAG does not have any requirements\\nfor this ability. As you can imagine, when more\\nadvanced LLMs are available in the future, they\\ncan be coupled with CRAG easily, while additional\\ninstruction tuning is still necessary for Self-RAG.LLaMA2-hf-7b SelfRAG-LLaMA2-7b\\nCRAG 54.9 59.8\\nw/o. Correct 53.2 58.3\\nw/o. Incorrect 54.4 59.5\\nw/o. Ambiguous 54.0 59.0\\nSelf-CRAG 49.0 61.8\\nw/o. Correct 43.6 59.6\\nw/o. Incorrect 47.7 60.8\\nw/o. Ambiguous 48.1 61.5\\nTable 2: Ablation study for removing each single action\\non the PopQA dataset in terms of accuracy.\\nLLaMA2-hf-7b SelfRAG-LLaMA2-7b\\nCRAG 54.9 59.8\\nw/o. refinement 49.8 54.2\\nw/o. rewriting 51.7 56.2\\nw/o. selection 50.9 58.6\\nSelf-CRAG 49.0 61.8\\nw/o. refinement 35.9 52.2\\nw/o. rewriting 37.2 58.4\\nw/o. selection 24.9 57.9\\nTable 3: Ablation study for removing each knowledge\\nutilization operation on the PopQA in terms of accuracy.\\n5.4 Ablation Study\\nThe impact of each triggered action. To fur-\\nther verify the effectiveness of triggered actions\\ndesigned in the retrieval evaluator, ablation tests\\nfor removing each single action in the proposed\\nmethod were conducted as shown in Table 2.\\nEvaluations on the PopQA dataset were conducted\\nto demonstrate the performance change in terms of\\naccuracy. Specifically, when the action Correct\\norIncorrect was removed, it was merged with\\nAmbiguous so that the proportion that originally\\ntriggered Correct orIncorrect would trigger\\nAmbiguous . On the other hand, when the action\\nAmbiguous was removed, there was only one\\nthreshold against which all input queries clearly\\ntriggered Correct orIncorrect . From these\\nresults, it can be seen that there was a performance\\ndrop no matter which action was removed, illustrat-\\ning that each action contributed to improving the\\nrobustness of generation. To further illustrate the', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 7}),\n",
       "  Document(page_content='drop no matter which action was removed, illustrat-\\ning that each action contributed to improving the\\nrobustness of generation. To further illustrate the\\nstudy, experiments are also conducted by triggering\\nonly one action once, and the results shown in the\\nappendix also prove the consistency.', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 7}),\n",
       "  Document(page_content='Accuracy\\nOur Retrieval Evaluator (T5-based) 84.3\\nChatGPT 58.0\\nChatGPT-CoT 62.4\\nChatGPT-few-shot 64.7\\nTable 4: Evaluation of our retrieval evaluator and\\nChatGPT for the retrieval results on the PopQA dataset.\\nThe impact of each knowledge utilization oper-\\nation. Table 3 illustrated how the performance\\nchanged if a key knowledge utilization operation\\nwas ablated. Evaluations on the PopQA dataset in\\nterms of accuracy were conducted by individually\\nremoving the knowledge utilization operations of\\ndocument refinement, search query rewriting, and\\nexternal knowledge selection. Removing document\\nrefinement denoted that the original retrieved docu-\\nments were directly fed to the following generator,\\nas in most existing works. Additionally, removing\\nsearch query rewriting denoted that questions were\\nnot rewritten into queries consisting of keywords\\nduring knowledge searching. Eventually, removing\\nknowledge selection denoted that all searched con-\\ntent of web pages was all regarded as the external\\nknowledge without selection. These results help\\nderive the findings that the performance of the\\nfinal system degraded no matter which knowledge\\nutilization operation was removed, revealing that\\neach knowledge utilization operation contributed\\nto improving the utilization of knowledge.\\n5.5 Accuracy of the Retrieval Evaluator\\nThe quality of the retrieval evaluator significantly\\ndetermined the performance of the entire system.\\nGiven the document retrieval results, we assessed\\nwhether the retrieval evaluator can accurately\\ndetermine the overall quality of these results. The\\nassessment accuracy on the PopQA dataset of\\nour retrieval evaluator and the commercial LLM\\nChatGPT on the document retrieval results was\\nshown in Table 4. The prompts of ChatGPT ,\\nChatGPT-CoT , and ChatGPT-few-shot used in our\\nexperiments can be referred to in Appendix A.\\nResults reveal that the lightweight T5-based re-\\ntrieval evaluator significantly outperformed the\\ncompetitive ChatGPT in all settings.\\n5.6 Robustness to Retrieval Performance\\nTo further verify the robustness of the proposed\\nmethod to retrieval performance, we studied how\\nthe generation performance changed given different\\n69.8\\n(Actual)60 50 40 30 20 10\\nAccuracy of retrieval203040506070 Accuracy of generation\\nno retrievalSelf-RAG Self-CRAGFigure 3: The generation performance of Self-RAG\\nand Self-CRAG given different retrieval performance\\non the PopQA dataset with SelfRAG-LLaMA-7b. The\\nlower horizontal line demonstrates the performance of\\nthe generator without retrieval.\\nLLaMA2-hf-7b SelfRAG-LLaMA2-7b\\nPopQA\\nCRAG 54.9 59.8\\nRAG 50.5 52.8\\nRAG w. web 52.2 53.8\\nSelf-CRAG 49.0 61.8\\nSelf-RAG 29.0 54.9\\nSelf-RAG w. web 24.9 57.9\\nTable 5: Comparison results between CRAG , Self-\\nCRAG and RAG, Self-RAG with the same input in\\nterms of accuracy.\\nretrieval performance. A part of accurate retrieval\\nresults were deliberately removed at random to\\nimitate a low-quality retriever and evaluate how\\nthe performance changed. Figure 3 demonstrated\\nthe performance change of Self-RAG and Self-\\nCRAG on the PopQA dataset. It can be seen\\nthat the generation performance of Self-RAG and\\nSelf-CRAG dropped as the retrieval performance\\ndropped, indicating that the generator relied heavily\\non the quality of the retriever. Furthermore, as\\nthe retrieval performance dropped, the generation\\nperformance of Self-CRAG dropped more slightly\\nthan that of Self-RAG. These results imply the\\nsuperiority of Self-CRAG over Self-RAG on en-\\nhancing the robustness to retrieval performance.\\n5.7 Consistent Supplementation of Web\\nSearch Knowledge\\nThis paper highlights the necessity of enhancing\\nthe retrieved context by incorporating additional\\ninformation when the initial retrieval results are\\nirrelevant and unreliable. Meanwhile, it is also\\ncrucial to confirm that the primary improvements\\nin our method stem from the self-correction mech-', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 8}),\n",
       "  Document(page_content='TFLOPs per token executing time(s)\\nRAG 26.5 0.363\\nCRAG 27.2 0.512\\nSelf-RAG 26.5 ∼132.4 0.741\\nSelf-CRAG 27.2 ∼80.2 0.908\\nTable 6: computational overhead assessment of RAG,\\nCRAG , Self-CRAG, and Self-RAG about FLOPs per\\ntoken on GPUs and executing time per instance. The\\nupper bound of Self-CRAG is lower because only three\\npassages are provided as input (correct, incorrect and\\nambiguous content). All the data in the table only\\nrepresents a rough estimate of the generation phase, the\\nretrieval and data-processing stages are not included.\\nanism, rather than solely from the supplementary\\ninformation obtained through web searches. To\\nfurther demonstrate the effectiveness of the pro-\\nposed self-correction mechanism, both RAG and\\nSelf-RAG were consistently supplemented with\\nweb search knowledge to ensure they had access\\nto the same scope of the retrieved knowledge.\\nThe results in Table 5 show that consistently\\nsupplementing RAG or Self-RAG with web search\\nknowledge can improve the performance in most\\ncases (except Self-RAG w. web using the original\\nLLaMA2 model), though the improvement remains\\nlimited. Furthermore, augmenting RAG or Self-\\nRAG with the proposed self-correction mechanism\\nsignificantly outperformed the models consistently\\nsupplemented with web search knowledge in all\\ncases. This finding confirms that the observed\\nadvancements are primarily attributable to the\\nproposed self-correction mechanism.\\n5.8 Computational Overhead Analysis\\nTo illustrate that our self-correction mechanism\\nserves as a lightweight, plug-and-play solution\\nfor various RAG-based frameworks, we measured\\nthe computational overhead. FLOPs prediction\\nformulas in Narayanan et al. (2021) were employed,\\nwith the results presented in Table 6 which shows\\nthe predicted FLOPs per token on GPUs. Due to\\nthe adaptive nature of Self-RAG, which varies its\\ngeneration strategies based on input, the compu-\\ntational overhead cannot be precisely determined.\\nTherefore, we present an estimated range instead.\\nAdditionally, we conducted the experiments on\\nPopQA to assess the average execution time per\\ninstance in practice, as detailed in Table 6. The\\nfindings indicate that the self-correction mecha-\\nnism incurs only modest computational overheadwhile significantly enhancing performance, thereby\\nvalidating its lightweight nature.\\n6 Conclusion & Limitation\\nThis paper studies the problem where RAG-based\\napproaches are challenged if retrieval goes wrong,\\nthereby exposing inaccurate and misleading knowl-\\nedge to generative LMs. Corrective Retrieval\\nAugmented Generation is proposed to improve the\\nrobustness of generation. Essentially, a lightweight\\nretrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly\\nimproved the ability of automatic self-correction\\nand efficient utilization of retrieved documents.\\nExperiments extensively demonstrate its adaptabil-\\nity to RAG-based approaches as well as general-\\nizability across short- and long-form generation\\ntasks. While we primarily proposed to improve the\\nRAG framework from a corrective perspective and\\nCRAG can be seamlessly coupled with various\\nRAG-based approaches, fine-tuning an external\\nretrieval evaluator is inevitable. How to eliminate\\nthis external evaluator and equip LLMs with better\\nretrieval evaluation capabilities will be our future\\nwork.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern,\\nGaurav Mishra, Erica Moreira, Mark Omernick,\\nKevin Robinson, Sebastian Ruder, et al. 2023. PaLM\\n2 technical report. CoRR , abs/2305.10403.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2024. Self-rag: Learning to\\nretrieve, generate, and critique through self-reflection.', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 9}),\n",
       "  Document(page_content='Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2024. Self-rag: Learning to\\nretrieve, generate, and critique through self-reflection.\\nInThe Twelfth International Conference on Learning\\nRepresentations, ICLR 2024, Vienna, Austria, May\\n7-11, 2024 . OpenReview.net.\\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee,\\nWenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia,\\nZiwei Ji, Tiezheng Yu, Willy Chung, Quyet V . Do,\\nYan Xu, and Pascale Fung. 2023. A multitask,\\nmultilingual, multimodal evaluation of chatgpt on\\nreasoning, hallucination, and interactivity. pages\\n675–718.\\nSumithra Bhakthavatsalam, Daniel Khashabi, Tushar\\nKhot, Bhavana Dalvi Mishra, Kyle Richardson,\\nAshish Sabharwal, Carissa Schoenick, Oyvind', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 9}),\n",
       "  Document(page_content='Tafjord, and Peter Clark. 2021. Think you have\\nsolved direct-answer question answering? try arc-da,\\nthe direct-answer AI2 reasoning challenge. CoRR ,\\nabs/2102.03315.\\nTom B Brown, Benjamin Mann, Nick Ryder, et al.\\n2020. Language models are few-shot learners. In\\nAdvances in neural information processing systems ,\\npages 1877–1901.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer,\\nVinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\\nDavid Dohan, Shivani Agrawal, Mark Omernick,\\nAndrew M. Dai, Thanumalayan Sankaranarayana\\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\\nRewon Child, Oleksandr Polozov, Katherine Lee,\\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\\nand Noah Fiedel. 2023. Palm: Scaling language\\nmodeling with pathways. J. Mach. Learn. Res. ,\\n24:240:1–240:113.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\\nJason Weston. 2024. Chain-of-verification reduces\\nhallucination in large language models. pages 3563–\\n3578.\\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. 2023. Alpaca-\\nfarm: A simulation framework for methods that learn\\nfrom human feedback. CoRR , abs/2305.14387.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\\nand Ming-Wei Chang. 2020. Retrieval augmented\\nlanguage model pre-training. In Proceedings of the\\n37th International Conference on Machine Learning,\\nICML 2020, 13-18 July 2020, Virtual Event , volume\\n119 of Proceedings of Machine Learning Research ,\\npages 3929–3938. PMLR.\\nGautier Izacard, Mathilde Caron, Lucas Hosseini,\\nSebastian Riedel, Piotr Bojanowski, Armand Joulin,\\nand Edouard Grave. 2022. Unsupervised dense\\ninformation retrieval with contrastive learning. Trans.\\nMach. Learn. Res. , 2022.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\\nMadotto, and Pascale Fung. 2023. Survey of\\nhallucination in natural language generation. ACM\\nComput. Surv. , 55(12):248:1–248:38.Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing\\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\\nJamie Callan, and Graham Neubig. 2023. Active\\nretrieval augmented generation. In Proceedings\\nof the 2023 Conference on Empirical Methods\\nin Natural Language Processing, EMNLP 2023,\\nSingapore, December 6-10, 2023 , pages 7969–7992.\\nAssociation for Computational Linguistics.\\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin\\nPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo\\nHa, and Jinwoo Shin. 2024. Sure: Summarizing\\nretrievals using answer candidates for open-domain\\nQA of llms. In The Twelfth International Conference\\non Learning Representations, ICLR 2024, Vienna,\\nAustria, May 7-11, 2024 . OpenReview.net.\\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\\n2022. Internet-augmented dialogue generation. In\\nProceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), ACL 2022, Dublin, Ireland, May\\n22-27, 2022 , pages 8460–8478. Association for\\nComputational Linguistics.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra\\nPiktus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 10}),\n",
       "  Document(page_content='Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in\\nNeural Information Processing Systems 33: Annual\\nConference on Neural Information Processing\\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\\nvirtual .\\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\\nLemao Liu. 2022. A survey on retrieval-augmented\\ntext generation. CoRR , abs/2202.01110.\\nYanming Liu, Xinyue Peng, Xuhong Zhang, Weihao\\nLiu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024.\\nRA-ISF: learning to answer and understand from\\nretrieval augmentation via iterative self-feedback.\\nInFindings of the Association for Computational\\nLinguistics, ACL 2024, Bangkok, Thailand and\\nvirtual meeting, August 11-16, 2024 , pages 4730–\\n4749. Association for Computational Linguistics.\\nHongyin Luo, Tianhua Zhang, Yung-Sung Chuang,\\nYuan Gong, Yoon Kim, Xixin Wu, Helen Meng, and\\nJames R. Glass. 2023. Search augmented instruction\\nlearning. In Findings of the Association for\\nComputational Linguistics: EMNLP 2023, Singapore,\\nDecember 6-10, 2023 , pages 3717–3729. Association\\nfor Computational Linguistics.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\\nDas, Daniel Khashabi, and Hannaneh Hajishirzi.\\n2023. When not to trust language models:\\nInvestigating effectiveness of parametric and non-\\nparametric memories. In Proceedings of the 61st\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), ACL 2023,\\nToronto, Canada, July 9-14, 2023 , pages 9802–9822.\\nAssociation for Computational Linguistics.', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 10}),\n",
       "  Document(page_content='Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of factual\\nprecision in long form text generation. In\\nProceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP\\n2023, Singapore, December 6-10, 2023 , pages 12076–\\n12100. Association for Computational Linguistics.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend, Kevin\\nLeyton-Brown, Amnon Shashua, and Yoav Shoham.\\n2023. Generating benchmarks for factuality evalua-\\ntion of language models. CoRR , abs/2307.06908.\\nDeepak Narayanan, Mohammad Shoeybi, Jared\\nCasper, Patrick LeGresley, Mostofa Patwary, Vijay\\nKorthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\\nJulie Bernauer, Bryan Catanzaro, Amar Phanishayee,\\nand Matei Zaharia. 2021. Efficient large-scale\\nlanguage model training on GPU clusters using\\nmegatron-lm. In International Conference for\\nHigh Performance Computing, Networking, Storage\\nand Analysis, SC 2021, St. Louis, Missouri, USA,\\nNovember 14-19, 2021 , page 58. ACM.\\nOpenAI. 2023. GPT-4 technical report. CoRR ,\\nabs/2303.08774.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex\\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\\nLuke Miller, Maddie Simens, Amanda Askell, Peter\\nWelinder, Paul F. Christiano, Jan Leike, and Ryan\\nLowe. 2022. Training language models to follow\\ninstructions with human feedback. In NeurIPS .\\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\\nPatrick S. H. Lewis, Barlas Oguz, Edouard Grave,\\nWen-tau Yih, and Sebastian Riedel. 2021. The web\\nis your oyster - knowledge-intensive NLP against a\\nvery large web corpus. CoRR , abs/2112.09924.\\nChengwei Qin, Aston Zhang, Zhuosheng Zhang,\\nJiaao Chen, Michihiro Yasunaga, and Diyi Yang.\\n2023. Is chatgpt a general-purpose natural language\\nprocessing task solver? In Proceedings of the\\n2023 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2023, Singapore,\\nDecember 6-10, 2023 , pages 1339–1384. Association\\nfor Computational Linguistics.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. J. Mach. Learn. Res. , 21:140:1–140:67.\\nMd. Rashad Al Hasan Rony, Ricardo Usbeck, and\\nJens Lehmann. 2022. Dialokg: Knowledge-structure\\naware task-oriented dialogue generation. In Findings\\nof the Association for Computational Linguistics:NAACL 2022, Seattle, WA, United States, July\\n10-15, 2022 , pages 2557–2571. Association for\\nComputational Linguistics.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Eric Hambro, Luke\\nZettlemoyer, Nicola Cancedda, and Thomas Scialom.\\n2023. Toolformer: Language models can teach\\nthemselves to use tools.\\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\\nScales, David Dohan, Ed H. Chi, Nathanael Schärli,\\nand Denny Zhou. 2023. Large language models\\ncan be easily distracted by irrelevant context. In\\nProceedings of the 40th International Conference\\non Machine Learning , volume 202 of Proceedings\\nof Machine Learning Research , pages 31210–31227.\\nPMLR.\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe\\nKiela, and Jason Weston. 2021. Retrieval\\naugmentation reduces hallucination in conversation.\\nInFindings of the Association for Computational\\nLinguistics: EMNLP 2021, Virtual Event / Punta\\nCana, Dominican Republic, 16-20 November, 2021 ,\\npages 3784–3803. Association for Computational\\nLinguistics.\\nChao-Hong Tan, Jia-Chen Gu, Chongyang Tao, Zhen-\\nHua Ling, Can Xu, Huang Hu, Xiubo Geng,\\nand Daxin Jiang. 2022. Tegtok: Augmenting\\ntext generation via task-specific and open-world\\nknowledge. In Findings of the Association for\\nComputational Linguistics: ACL 2022, Dublin,', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 11}),\n",
       "  Document(page_content='and Daxin Jiang. 2022. Tegtok: Augmenting\\ntext generation via task-specific and open-world\\nknowledge. In Findings of the Association for\\nComputational Linguistics: ACL 2022, Dublin,\\nIreland, May 22-27, 2022 , pages 1597–1609.\\nAssociation for Computational Linguistics.\\nS. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman,\\nVinija Jain, Anku Rani, Vipula Rawte, Aman Chadha,\\nand Amitava Das. 2024. A comprehensive survey of\\nhallucination mitigation techniques in large language\\nmodels. CoRR , abs/2401.01313.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023a. Llama: Open\\nand efficient foundation language models. CoRR ,\\nabs/2302.13971.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter\\nAlbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, et al. 2023b.\\nLlama 2: Open foundation and fine-tuned chat\\nmodels. CoRR , abs/2307.09288.\\nZihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian\\nMa, and Yitao Liang. 2024. RAT: retrieval\\naugmented thoughts elicit context-aware reasoning\\nin long-horizon generation. CoRR , abs/2403.05313.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\\nBerant. 2024. Making retrieval-augmented language\\nmodels robust to irrelevant context.', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 11}),\n",
       "  Document(page_content='Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\\nDanny Fox, Helen Meng, and James R. Glass. 2023a.\\nInterpretable unified language checking. CoRR ,\\nabs/2304.03728.\\nTianjun Zhang, Shishir G. Patil, Naman Jain, Sheng\\nShen, Matei Zaharia, Ion Stoica, and Joseph E.\\nGonzalez. 2024. RAFT: adapting language model to\\ndomain specific RAG. CoRR , abs/2403.10131.\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\\nBi, Freda Shi, and Shuming Shi. 2023b. Siren’s song\\nin the AI ocean: A survey on hallucination in large\\nlanguage models. CoRR , abs/2309.01219.\\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\\nDacheng Tao. 2023. Can chatgpt understand too? A\\ncomparative study on chatgpt and fine-tuned BERT.\\nCoRR , abs/2302.10198.', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 12}),\n",
       "  Document(page_content='A Task Prompts\\nThe prompts for generating knowledge keywords\\nas web search queries were illustrated in Table 7.\\nTable 7: The few-shot prompt to GPT-3.5 Turbo for\\ngenerating knowledge keywords as web search queries.\\nExtract at most three keywords separated by comma from\\nthe following dialogues and questions as queries for the\\nweb search, including topic background within dialogues\\nand main intent within questions.\\nquestion: What is Henry Feilden’s occupation?\\nquery: Henry Feilden, occupation\\nquestion: In what city was Billy Carlson born?\\nquery: city, Billy Carlson, born\\nquestion: What is the religion of John Gwynn?\\nquery: religion of John Gwynn\\nquestion: What sport does Kiribati men’s national\\nbasketball team play?\\nquery: sport, Kiribati men’s national basketball team play\\nquestion: [question]\\nquery:\\nThe prompts to instruct ChatGPT as the evalua-\\ntor were illustrated in Table 8, Table 9, and Table 10\\nrespectively.\\nTable 8: The direct prompt to GPT-3.5 Turbo as the\\nevaluator.\\nGiven a question, does the following document have exact\\ninformation to answer the question? Answer yes or no\\nonly.\\nQuestion: [question]\\nDocument: [document]\\nTable 9: The prompt to GPT-3.5 Turbo with Chain-of-\\nThought as the evaluator.\\nGiven a question, does the following document have exact\\ninformation to answer the question?\\nQuestion: [question]\\nDocument: [document]\\nThink Step by step, and answer with yes or no only.Table 10: The few-shot prompt to GPT-3.5 Turbo as the\\nevaluator.\\nGiven a question, does the following document have exact\\ninformation to answer the question? Answer yes or no\\nonly.\\nQuestion: In what city was Abraham Raimbach born?\\nDocument: Bancroft was born on November 25, 1839\\nin New Ipswich, New Hampshire to James Bancroft and\\nSarah Kimball. At an early age he was cared for by Mr.\\nand Mrs. Patch of Ashby, Massachusetts, the neighboring\\ntown. While not legally adopted, they named him Cecil\\nFranklin Patch Bancroft, adding Franklin Patch after the\\nson Mr. and Mrs. Patch had who recently died. He\\nattended public schools in Ashby as well as the Appleton\\nAcademy in New Ipswich. He entered Dartmouth College\\nin 1856 at the age of sixteen and graduated in 1860 near\\nthe top of his class. Bancroft continued his education as he\\nbegan his career in teaching. He took classes at the Union\\nTheological Seminary in New York City during the 1864-\\n65 academic year. While there he was a member of the\\nUnited States Christian Commission, traveling to support\\nsoldiers during the Civil War. He then transferred to the\\nAndover Theological Seminary where he would graduate\\nin 1867.\\nAnswer: No.\\nQuestion: In what country is Wilcza Jama, Sokółka\\nCounty?\\nDocument: Wilcza Jama is a village in the administrative\\ndistrict of Gmina Sokółka, within Sokółka County,\\nPodlaskie V oivodeship, in north-eastern Poland, close to\\nthe border with Belarus.\\nAnswer: Yes.\\nQuestion: What sport does 2004 Legg Mason Tennis\\nClassic play?\\nDocument: The 2004 Legg Mason Tenis Classic was the\\n36th edition of this tennis tournament and was played\\non outdoor hard courts. The tournament was part of the\\nInternational Series of the 2004 ATP Tour. It was held at\\nthe William H.G. FitzGerald Tennis Center in Washington,\\nD.C. from August 16 through August 22, 2004.\\nAnswer: Yes.\\nQuestion: Who is the author of Skin?\\nDocument: The Skin We’re In: A Year of Black Resistance\\nand Power is a book by Desmond Cole published by\\nDoubleday Canada in 2020. The Skin We’re In describes\\nthe struggle against racism in Canada during the year 2017,\\nchronicling Cole’s role as an anti-racist activist and the\\nimpact of systemic racism in Canadian society. Among\\nthe events it discusses are the aftermath of the assault of\\nDafonte Miller in late 2016 and Canada 150. The work\\nargues that Canada is not immune to the anti-Black racism\\nthat characterizes American society. Due to an error by the\\npublisher, the initial printing of the book’s cover did not\\ninclude word ¨Blackïn the subtitle. The mistake was later', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 13}),\n",
       "  Document(page_content='that characterizes American society. Due to an error by the\\npublisher, the initial printing of the book’s cover did not\\ninclude word ¨Blackïn the subtitle. The mistake was later\\ncorrected. The book won the Toronto Book Award for 2020.\\nIn 2021, the book was nominated for the Shaughnessy\\nCohen Prize for Political Writing.\\nAnswer: No.\\nQuestion: [question]\\nDocument: [document]\\nAnswer:', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 13}),\n",
       "  Document(page_content='B Experiments\\nB.1 Tasks, Datasets and Metrics\\nCRAG was evaluated on four datasets, which are in\\npublic domain and licensed for research purposes,\\nincluding:\\nPopQA (Mallen et al., 2023) is a short -form\\ngeneration task. Generally, only one entity of\\nfactual knowledge is expected to be answered for\\neach single question. In our experiments, we\\nexactly followed the setting in Self-RAG (Asai\\net al., 2024) which evaluated methods on a long-tail\\nsubset consisting of 1,399 rare entity queries whose\\nmonthly Wikipedia page views are less than 100.\\nAccuracy was adopted as the evaluation metric.\\nBiography (Min et al., 2023) is a long-form\\ngeneration task that is tasked to generate a detailed\\nbiography about a certain entity. Following previ-\\nous work, FactScore (Min et al., 2023) was adopted\\nto evaluate the generated biographies.\\nPubHealth (Zhang et al., 2023a) is a task\\nin health care domain consisting of true-or-false\\nquestions. Claims are represented about health\\nwith factual information, and the model is tasked\\nto verify the authenticity and give the judgment.\\nAccuracy was adopted as the evaluation metric.\\nArc-Challenge (Bhakthavatsalam et al., 2021)\\nis a multiple-choice question task about some\\ndaily commonsense science phenomena. Given\\na scientific event that occurs in daily life, the model\\nis required to select the correct description among\\n3 or 4 optional choices. Accuracy was adopted as\\nthe evaluation metric as well.\\nB.2 Experiments compute Resources\\nWe used NVIDIA A800 80GB GPU for experi-\\nments. For LLaMA-2 (7B) generation, it occupies\\nover 40GB memory during inference. For T5-large\\n(0.77B) fine-tuning, it takes much less compared\\nwith LLaMA-2.\\nB.3 Implementation Details\\nRetrieval Evaluator: We fine-tuned the retrieval\\nevaluator based on the lightweight T5-large (Raffel\\net al., 2020) pre-trained model. The dataset we\\nused is the version provided by Self-RAG (Asai\\net al., 2024). Specifically, the original PopQA\\ndataset consists of 14k samples, 1,399 of which\\nwere used for testing following Self-RAG (Asai\\net al., 2024), and the remaining were used for\\nfine-tuning to avoid information leakage. Besides,\\nthe fine-tuned evaluator was transferred and alsoutilized on the Bio, Pub and ARC datasets during\\ninference. The label of positive samples was 1,\\nwhile that of negative ones was -1. At inference,\\nthe evaluator scored the relevance from -1 to 1 for\\neach document. The two confidence thresholds\\nfor triggering one of the three actions were set\\nempirically. Specifically, they were set as (0.59,\\n-0.99) in PopQA, (0.5, -0.91) in PubQA and Arc-\\nChallenge, as well as (0.95, -0.91) in Biography.\\nInternal Knowledge: To obtain fine-grained\\nretrieval results, we segmented the retrieved results\\ninto internal strips. If a retrieved result is as short as\\none or two sentences, it is regarded as an individual\\nstrip, otherwise, retrieval documents are required to\\nbe split into smaller units which generally consist\\nof a few sentences according to the total length.\\nThe scale is assumed to include an independent\\npiece of information, and the filtering is based on\\nthe segments. We directly adopted the evaluator\\nagain for knowledge strips filtering, and the top-k\\nis set to 5, filter threshold as -0.5.\\nExternal Knowledge: Google Search API was\\nadopted to search for the relevant URLs, top-k\\nis set to 5, and pages from Wikipedia will be\\nadded preferentially. The searched web pages\\nare generally in the form of HTML files, where\\ncontent is split with special tokens like <p>\\nand</p>. Thus an extra segmentation like the\\nknowledge refinement is not required, related\\nknowledge paragraphs can be directly selected with\\nthe evaluator similar to internal knowledge. In\\nthis way, the accuracy of the search outcomes can\\nbe ensured without compromising the quality and\\nrelevance of the information used for generation.\\nGenerator: AsCRAG is a plug-and-play\\nmethod, all generation models that can be uti-\\nlized in RAG fit our approach as well. To', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 14}),\n",
       "  Document(page_content='relevance of the information used for generation.\\nGenerator: AsCRAG is a plug-and-play\\nmethod, all generation models that can be uti-\\nlized in RAG fit our approach as well. To\\nbe consistent with baselines for comparison, we\\nadopted LLaMA2 (Touvron et al., 2023b) for the\\ngeneration. We first introduced the LLaMA2-hf-\\n7bfrom huggingface to generate responses. Since\\nSelf-RAG (Asai et al., 2024) fine-tuned LLaMA2\\nand reached a new state-of-the-art performance\\non several tasks, we further utilized the launched\\nmodel, SelfRAG-LLaMA2-7b , as a new generator to\\nbe consistent with their work and study the specific\\nimprovement of our method.\\nSelf-CRAG: To demonstrate that our plug-and-\\nplay approach can be utilized in other concurrent\\nstudies, we specifically designed to insert our\\nCRAG into the Self-RAG (Asai et al., 2024)\\nframework and named it Self-CRAG. Self-RAG', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 14}),\n",
       "  Document(page_content='Table 11: Ablation study for removing only a single\\naction on the PopQA dataset in terms of accuracy.\\nLLaMA2-hf-7b SelfRAG-LLaMA2-7b\\nCRAG 54.9 59.8\\nonly Correct 52.4 56.7\\nonly Incorrect 47.0 48.5\\nonly Ambiguous 52.7 58.0\\nSelf-CRAG 49.0 61.8\\nonly Correct 48.6 57.2\\nonly Incorrect 40.8 53.3\\nonly Ambiguous 44.9 59.8\\nis an advanced RAG approach that introduces a\\ncritic model to decide whether to retrieve and which\\nretrieved document to be referred for generation. It\\nmeets our demand for deciding which action to be\\ntriggered, thus we replaced the retrieved items in\\nSelf-RAG with our processed internal knowledge\\nforCorrect , external knowledge for Incorrect ,\\nand combined knowledge for Ambiguous .\\nB.4 More Detailed Results\\nAblation Study: The following results in Table 11\\ndemonstrate the ablation study by triggering one\\naction only for all instances.\\nB.5 Results on PubHealth and Arc-Challenge\\nIt is worth mentioning that the performance on\\nPubHealth based on LLaMA2-hf-7b was much\\nworse than others. We studied these cases and\\nfound that LLaMA2-hf-7b is relatively weak in\\ninstruction comprehension. Most of the cases\\nfail to generate True orFalse in such a binary-\\nquestion task, resulting in a low accuracy during\\nthe evaluation. This situation somewhat happens in\\nArc-Challenge as well, when the model is tasked\\nto generate the index of a candidate.', metadata={'source': 'https://arxiv.org/pdf/2401.15884.pdf', 'page': 15})],\n",
       " 'output_text': ' \\n\\nThe article presents a new approach, Corrective Retrieval Augmented Generation (CRAG), to improve the accuracy and reliability of language generation models by addressing the issue of irrelevant and inaccurate retrieved documents. CRAG includes a retrieval evaluator and a decompose-then-recompose algorithm to filter and refine retrieved information. Experiments show that CRAG significantly improves the performance of RAG-based approaches. The article also discusses related work on hallucinations in language models and the importance of incorporating additional information. Other topics covered include reducing hallucination in large language models, multitask evaluations, and the use of prompts for generating knowledge keywords.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_file_path = \"https://arxiv.org/pdf/2401.15884.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_file_path)\n",
    "docs = pdf_loader.load_and_split()\n",
    "llm = OpenAI()\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ed41ea-138a-4826-9f74-329a85a7281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "\n",
      "This article discusses the limitations of large language models (LLMs) and introduces a solution called Corrective Retrieval Augmented Generation (CRAG). CRAG uses a lightweight retrieval evaluator and web searches to improve the accuracy and robustness of LLMs in generating texts. It also includes a decompose-then-recompose algorithm to filter out irrelevant information. Experiments show that CRAG significantly improves the performance of retrieval-augmented generation approaches. It also discusses the issue of hallucinations in LLMs and how CRAG can help mitigate this problem. The article also references other related studies and papers in the field of retrieval-augmented text generation and LLMs. Finally, it presents the evaluation and results of CRAG on four datasets and compares it to other baseline models.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"'\\n\\nThis article discusses the limitations of large language models (LLMs) and introduces a solution called Corrective Retrieval Augmented Generation (CRAG). CRAG uses a lightweight retrieval evaluator and web searches to improve the accuracy and robustness of LLMs in generating texts. It also includes a decompose-then-recompose algorithm to filter out irrelevant information. Experiments show that CRAG significantly improves the performance of retrieval-augmented generation approaches. It also discusses the issue of hallucinations in LLMs and how CRAG can help mitigate this problem. The article also references other related studies and papers in the field of retrieval-augmented text generation and LLMs. Finally, it presents the evaluation and results of CRAG on four datasets and compares it to other baseline models.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca675ca-81fb-4d72-8f04-14a369759bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
